# Alert Configuration for Production Monitoring

groups:
  - name: application_alerts
    interval: 30s
    rules:
      # High Error Rate
      - alert: HighErrorRate
        expr: rate(application_errors_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
          runbook: "https://docs.example.com/runbooks/high-error-rate"

      # High Request Latency
      - alert: HighRequestLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High request latency detected"
          description: "95th percentile latency is {{ $value }}s"

      # Low Cache Hit Rate
      - alert: LowCacheHitRate
        expr: rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.7
        for: 15m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Cache hit rate below threshold"
          description: "Cache hit rate is {{ $value | humanizePercentage }}"

      # Database Connection Pool Exhaustion
      - alert: DatabaseConnectionPoolExhaustion
        expr: database_connection_pool_size{state="idle"} < 2
        for: 5m
        labels:
          severity: critical
          team: database
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "Only {{ $value }} idle connections remaining"

      # AI Model Slow Inference
      - alert: AIModelSlowInference
        expr: histogram_quantile(0.95, rate(ai_model_prediction_time_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
          team: ml
        annotations:
          summary: "AI model inference is slow"
          description: "Model {{ $labels.model_name }} inference time is {{ $value }}s"

  - name: infrastructure_alerts
    interval: 30s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: avg(system_cpu_usage_percent) > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is {{ $value }}%"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: (system_memory_usage_bytes{type="used"} / (system_memory_usage_bytes{type="used"} + system_memory_usage_bytes{type="available"})) > 0.9
        for: 10m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value | humanizePercentage }}"

      # Disk Space Low
      - alert: DiskSpaceLow
        expr: (system_disk_usage_bytes{type="free"} / (system_disk_usage_bytes{type="used"} + system_disk_usage_bytes{type="free"})) < 0.1
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Disk space critically low"
          description: "Only {{ $value | humanizePercentage }} disk space remaining on {{ $labels.mount_point }}"

      # Service Unavailable
      - alert: ServiceUnavailable
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          team: oncall
        annotations:
          summary: "Service is down"
          description: "{{ $labels.job }} has been down for more than 2 minutes"

  - name: business_alerts
    interval: 60s
    rules:
      # Low Document Processing Rate
      - alert: LowDocumentProcessingRate
        expr: rate(documents_processed_total[15m]) < 1
        for: 30m
        labels:
          severity: warning
          team: product
        annotations:
          summary: "Document processing rate is low"
          description: "Processing {{ $value }} documents per second"

      # High Arbitration Detection Failure Rate
      - alert: HighArbitrationDetectionFailure
        expr: rate(documents_processed_total{status="failed"}[5m]) / rate(documents_processed_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
          team: ml
        annotations:
          summary: "High failure rate in arbitration detection"
          description: "Failure rate is {{ $value | humanizePercentage }}"

      # SLA Violation
      - alert: SLAViolation
        expr: increase(sla_violations_total[1h]) > 5
        for: 5m
        labels:
          severity: critical
          team: management
        annotations:
          summary: "Multiple SLA violations detected"
          description: "{{ $value }} SLA violations in the last hour"

      # Low API Usage
      - alert: LowAPIUsage
        expr: rate(api_usage_total[1h]) < 10
        for: 2h
        labels:
          severity: info
          team: product
        annotations:
          summary: "API usage is unusually low"
          description: "API usage is {{ $value }} requests per hour"

  - name: security_alerts
    interval: 30s
    rules:
      # Suspicious Activity
      - alert: SuspiciousActivity
        expr: rate(application_errors_total{error_type="authentication"}[5m]) > 10
        for: 5m
        labels:
          severity: critical
          team: security
        annotations:
          summary: "Potential security threat detected"
          description: "High rate of authentication failures: {{ $value }} per second"

      # Unusual Traffic Pattern
      - alert: UnusualTrafficPattern
        expr: rate(http_requests_total[5m]) > 1000
        for: 5m
        labels:
          severity: warning
          team: security
        annotations:
          summary: "Unusual traffic pattern detected"
          description: "Request rate is {{ $value }} per second"

# Alert notification channels
receivers:
  - name: slack-critical
    slack_configs:
      - api_url: "${SLACK_WEBHOOK_URL}"
        channel: "#alerts-critical"
        title: "Critical Alert"
        text: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}"

  - name: pagerduty-oncall
    pagerduty_configs:
      - routing_key: "${PAGERDUTY_ROUTING_KEY}"
        description: "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}"

  - name: email-team
    email_configs:
      - to: "team@example.com"
        from: "alerts@example.com"
        smarthost: "smtp.example.com:587"
        auth_username: "${SMTP_USERNAME}"
        auth_password: "${SMTP_PASSWORD}"
        headers:
          Subject: "Alert: {{ .GroupLabels.alertname }}"

# Alert routing rules
route:
  group_by: ["alertname", "cluster", "service"]
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: slack-critical
  
  routes:
    - match:
        severity: critical
      receiver: pagerduty-oncall
      continue: true
      
    - match:
        severity: warning
      receiver: slack-critical
      
    - match:
        team: security
      receiver: email-team
      continue: true