"""
Demo test scenarios for multilingual arbitration detection.
Demonstrates accuracy across different languages and cultural contexts.
"""

import pytest
import time
import json
from typing import Dict, List, Any, Tuple
from unittest.mock import Mock
import uuid


class TestMultilingualDemo:
    """Demo tests for multilingual arbitration detection capabilities."""
    
    @pytest.fixture(autouse=True)
    def setup_method(self):
        """Setup for each test method."""
        self.demo_results = {}
        self.multilingual_metrics = {}
        self.setup_multilingual_test_data()
        
    def setup_multilingual_test_data(self):
        """Setup comprehensive multilingual test documents."""
        self.multilingual_documents = {
            "english": {
                "language": "English",
                "language_code": "en",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        ARBITRATION AGREEMENT
                        Any dispute arising out of or relating to this Agreement shall be settled by 
                        binding arbitration administered by the American Arbitration Association (AAA) 
                        in accordance with its Commercial Arbitration Rules. The arbitration shall be 
                        conducted in English in New York, New York.
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.9
                    },
                    "no_arbitration": {
                        "content": """
                        GOVERNING LAW AND JURISDICTION
                        This Agreement shall be governed by and construed in accordance with the laws 
                        of New York. Any disputes shall be resolved exclusively in the state and 
                        federal courts located in New York County.
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.9
                    }
                }
            },
            
            "spanish": {
                "language": "Spanish",
                "language_code": "es",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        ACUERDO DE ARBITRAJE
                        Cualquier disputa que surja de o se relacione con este Acuerdo serÃ¡ resuelta 
                        mediante arbitraje vinculante administrado por la AsociaciÃ³n Americana de 
                        Arbitraje (AAA) de acuerdo con sus Reglas de Arbitraje Comercial. El arbitraje 
                        se conducirÃ¡ en espaÃ±ol en Madrid, EspaÃ±a.
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.85
                    },
                    "no_arbitration": {
                        "content": """
                        LEY APLICABLE Y JURISDICCIÃ“N
                        Este Acuerdo se regirÃ¡ e interpretarÃ¡ de acuerdo con las leyes de EspaÃ±a. 
                        Cualquier disputa serÃ¡ resuelta exclusivamente en los tribunales estatales 
                        y federales ubicados en Madrid.
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.8
                    }
                }
            },
            
            "french": {
                "language": "French",
                "language_code": "fr",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        ACCORD D'ARBITRAGE
                        Tout diffÃ©rend dÃ©coulant de ou liÃ© Ã  cet Accord sera rÃ©solu par arbitrage 
                        contraignant administrÃ© par la Chambre de Commerce Internationale (CCI) 
                        conformÃ©ment Ã  son RÃ¨glement d'arbitrage. L'arbitrage se dÃ©roulera en 
                        franÃ§ais Ã  Paris, France.
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.85
                    },
                    "no_arbitration": {
                        "content": """
                        DROIT APPLICABLE ET JURIDICTION
                        Cet Accord sera rÃ©gi et interprÃ©tÃ© selon les lois franÃ§aises. Tout diffÃ©rend 
                        sera rÃ©solu exclusivement devant les tribunaux compÃ©tents de Paris.
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.8
                    }
                }
            },
            
            "german": {
                "language": "German",
                "language_code": "de",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        SCHIEDSVEREINBARUNG
                        Alle Streitigkeiten, die aus diesem Vertrag entstehen oder damit in 
                        Verbindung stehen, werden durch verbindliche Schiedsgerichtsbarkeit 
                        beigelegt, die von der Deutschen Institution fÃ¼r Schiedsgerichtsbarkeit 
                        (DIS) gemÃ¤ÃŸ ihrer Schiedsgerichtsordnung verwaltet wird.
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.85
                    },
                    "no_arbitration": {
                        "content": """
                        ANWENDBARES RECHT UND GERICHTSSTAND
                        Dieser Vertrag unterliegt deutschem Recht. Alle Streitigkeiten werden 
                        ausschlieÃŸlich vor den zustÃ¤ndigen Gerichten in Berlin beigelegt.
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.8
                    }
                }
            },
            
            "chinese_simplified": {
                "language": "Chinese (Simplified)",
                "language_code": "zh-CN",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        ä»²è£åè®®
                        å› æœ¬åè®®äº§ç”Ÿæˆ–ä¸Žæœ¬åè®®æœ‰å…³çš„ä»»ä½•äº‰è®®ï¼Œå‡åº”é€šè¿‡ä¸­å›½å›½é™…ç»æµŽè´¸æ˜“ä»²è£å§”å‘˜ä¼š
                        (CIETAC) æ ¹æ®å…¶ä»²è£è§„åˆ™è¿›è¡Œçš„æœ‰çº¦æŸåŠ›çš„ä»²è£æ¥è§£å†³ã€‚ä»²è£å°†åœ¨ä¸­å›½åŒ—äº¬è¿›è¡Œï¼Œ
                        ä½¿ç”¨ä¸­æ–‡è¿›è¡Œã€‚
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.8
                    },
                    "no_arbitration": {
                        "content": """
                        é€‚ç”¨æ³•å¾‹å’Œç®¡è¾–æƒ
                        æœ¬åè®®å—ä¸­åŽäººæ°‘å…±å’Œå›½æ³•å¾‹ç®¡è¾–å’Œè§£é‡Šã€‚ä»»ä½•äº‰è®®å°†åœ¨ä¸­å›½åŒ—äº¬çš„æœ‰ç®¡è¾–æƒçš„
                        æ³•é™¢ç‹¬å®¶è§£å†³ã€‚
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.75
                    }
                }
            },
            
            "japanese": {
                "language": "Japanese",
                "language_code": "ja",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        ä»²è£åˆæ„
                        æœ¬å¥‘ç´„ã‹ã‚‰ç”Ÿã˜ã‚‹åˆã¯æœ¬å¥‘ç´„ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®ç´›äº‰ã¯ã€æ—¥æœ¬å•†äº‹ä»²è£å”ä¼š
                        (JCAA)ã®ä»²è£è¦å‰‡ã«å¾“ã„ã€æ‹˜æŸåŠ›ã®ã‚ã‚‹ä»²è£ã«ã‚ˆã‚Šè§£æ±ºã•ã‚Œã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚
                        ä»²è£ã¯æ—¥æœ¬ã®æ±äº¬ã§æ—¥æœ¬èªžã«ã‚ˆã‚Šè¡Œã‚ã‚Œã¾ã™ã€‚
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.8
                    },
                    "no_arbitration": {
                        "content": """
                        æº–æ‹ æ³•ãŠã‚ˆã³ç®¡è½„
                        æœ¬å¥‘ç´„ã¯æ—¥æœ¬æ³•ã«æº–æ‹ ã—ã€ã“ã‚Œã«å¾“ã£ã¦è§£é‡ˆã•ã‚Œã¾ã™ã€‚ã™ã¹ã¦ã®ç´›äº‰ã¯æ±äº¬åœ°æ–¹
                        è£åˆ¤æ‰€ã®å°‚å±žç®¡è½„ã«ã‚ˆã‚Šè§£æ±ºã•ã‚Œã¾ã™ã€‚
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.75
                    }
                }
            },
            
            "portuguese": {
                "language": "Portuguese",
                "language_code": "pt",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        ACORDO DE ARBITRAGEM
                        Qualquer disputa decorrente ou relacionada a este Acordo serÃ¡ resolvida 
                        atravÃ©s de arbitragem vinculativa administrada pela CÃ¢mara de Arbitragem 
                        de SÃ£o Paulo (CASP) de acordo com suas Regras de Arbitragem. A arbitragem 
                        serÃ¡ conduzida em portuguÃªs em SÃ£o Paulo, Brasil.
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.85
                    },
                    "no_arbitration": {
                        "content": """
                        LEI APLICÃVEL E JURISDIÃ‡ÃƒO
                        Este Acordo serÃ¡ regido e interpretado de acordo com as leis brasileiras. 
                        Qualquer disputa serÃ¡ resolvida exclusivamente nos tribunais competentes 
                        de SÃ£o Paulo.
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.8
                    }
                }
            },
            
            "arabic": {
                "language": "Arabic",
                "language_code": "ar",
                "documents": {
                    "clear_arbitration": {
                        "content": """
                        Ø§ØªÙØ§Ù‚ÙŠØ© Ø§Ù„ØªØ­ÙƒÙŠÙ…
                        Ø£ÙŠ Ù†Ø²Ø§Ø¹ ÙŠÙ†Ø´Ø£ Ø¹Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø§ØªÙØ§Ù‚ÙŠØ© Ø£Ùˆ ÙŠØªØ¹Ù„Ù‚ Ø¨Ù‡Ø§ Ø³ÙŠØªÙ… Ø­Ù„Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ­ÙƒÙŠÙ… Ø§Ù„Ù…Ù„Ø²Ù… 
                        Ø§Ù„Ø°ÙŠ ÙŠØ¯ÙŠØ±Ù‡ Ù…Ø±ÙƒØ² Ø§Ù„ØªØ­ÙƒÙŠÙ… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ù„Ø¯ÙˆÙ„ Ù…Ø¬Ù„Ø³ Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠ ÙˆÙÙ‚Ø§Ù‹ Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­ÙƒÙŠÙ… 
                        Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù‡. Ø³ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ­ÙƒÙŠÙ… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø¯Ø¨ÙŠØŒ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©.
                        """,
                        "expected_arbitration": True,
                        "confidence_threshold": 0.8
                    },
                    "no_arbitration": {
                        "content": """
                        Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø·Ø¨Ù‚ ÙˆØ§Ù„Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ
                        ØªØ­ÙƒÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø§ØªÙØ§Ù‚ÙŠØ© ÙˆØªÙØ³Ø± ÙˆÙÙ‚Ø§Ù‹ Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©. 
                        Ø³ÙŠØªÙ… Ø­Ù„ Ø£ÙŠ Ù†Ø²Ø§Ø¹ Ø­ØµØ±ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙƒÙ… Ø§Ù„Ù…Ø®ØªØµØ© ÙÙŠ Ø¯Ø¨ÙŠ.
                        """,
                        "expected_arbitration": False,
                        "confidence_threshold": 0.75
                    }
                }
            }
        }
        
    def test_english_baseline_accuracy(self):
        """Demo: Establish baseline accuracy with English documents."""
        print(f"\n=== DEMO: English Baseline Accuracy ===")
        
        language_data = self.multilingual_documents["english"]
        results = []
        
        for doc_type, document in language_data["documents"].items():
            print(f"\nTesting English {doc_type}:")
            
            # Analyze document
            analysis = self._simulate_multilingual_analysis(
                document["content"], 
                language_data["language_code"]
            )
            
            # Validate results
            correct_prediction = analysis["has_arbitration"] == document["expected_arbitration"]
            confidence_met = analysis["confidence"] >= document["confidence_threshold"]
            
            result = {
                "document_type": doc_type,
                "language": language_data["language"],
                "expected": document["expected_arbitration"],
                "predicted": analysis["has_arbitration"],
                "confidence": analysis["confidence"],
                "correct_prediction": correct_prediction,
                "confidence_met": confidence_met,
                "language_confidence": analysis["language_confidence"],
                "detected_language": analysis["detected_language"]
            }
            
            results.append(result)
            
            status = "âœ“" if correct_prediction and confidence_met else "âœ—"
            print(f"{status} Prediction: {analysis['has_arbitration']} (confidence: {analysis['confidence']:.2f})")
            print(f"   Language detection: {analysis['detected_language']} ({analysis['language_confidence']:.2f})")
            
        # Calculate baseline metrics
        accuracy = sum(1 for r in results if r["correct_prediction"]) / len(results)
        confidence_score = sum(1 for r in results if r["confidence_met"]) / len(results)
        
        print(f"\nðŸ“Š English Baseline Results:")
        print(f"   Accuracy: {accuracy:.2%}")
        print(f"   Confidence Score: {confidence_score:.2%}")
        print(f"   Language Detection: 100%")  # Should always detect English correctly
        
        # Validation
        assert accuracy >= 0.95, f"English baseline accuracy too low: {accuracy:.2%}"
        assert confidence_score >= 0.9, f"English confidence score too low: {confidence_score:.2%}"
        
        self.multilingual_metrics["english_baseline"] = {
            "accuracy": accuracy,
            "confidence_score": confidence_score,
            "results": results
        }
        
        self.demo_results["english_baseline"] = "PASS"
        
    def test_romance_languages_accuracy(self):
        """Demo: Test accuracy on Romance languages (Spanish, French, Portuguese)."""
        print(f"\n=== DEMO: Romance Languages Accuracy Test ===")
        
        romance_languages = ["spanish", "french", "portuguese"]
        all_results = []
        language_results = {}
        
        for lang_key in romance_languages:
            language_data = self.multilingual_documents[lang_key]
            results = []
            
            print(f"\nTesting {language_data['language']}:")
            
            for doc_type, document in language_data["documents"].items():
                # Analyze document
                analysis = self._simulate_multilingual_analysis(
                    document["content"], 
                    language_data["language_code"]
                )
                
                # Validate results
                correct_prediction = analysis["has_arbitration"] == document["expected_arbitration"]
                confidence_met = analysis["confidence"] >= document["confidence_threshold"]
                language_detected = analysis["detected_language"].lower() == language_data["language"].lower()
                
                result = {
                    "document_type": doc_type,
                    "language": language_data["language"],
                    "expected": document["expected_arbitration"],
                    "predicted": analysis["has_arbitration"],
                    "confidence": analysis["confidence"],
                    "correct_prediction": correct_prediction,
                    "confidence_met": confidence_met,
                    "language_detected": language_detected,
                    "cross_language_keywords": analysis["cross_language_keywords"]
                }
                
                results.append(result)
                all_results.append(result)
                
                status = "âœ“" if correct_prediction and confidence_met else "âœ—"
                print(f"  {status} {doc_type}: {analysis['has_arbitration']} ({analysis['confidence']:.2f})")
                
            # Calculate language-specific metrics
            accuracy = sum(1 for r in results if r["correct_prediction"]) / len(results)
            language_detection = sum(1 for r in results if r["language_detected"]) / len(results)
            
            language_results[lang_key] = {
                "accuracy": accuracy,
                "language_detection": language_detection,
                "results": results
            }
            
            print(f"   Language accuracy: {accuracy:.2%}")
            print(f"   Language detection: {language_detection:.2%}")
            
        # Calculate overall Romance language metrics
        overall_accuracy = sum(1 for r in all_results if r["correct_prediction"]) / len(all_results)
        overall_language_detection = sum(1 for r in all_results if r["language_detected"]) / len(all_results)
        
        print(f"\nðŸ“Š Romance Languages Overall Results:")
        print(f"   Overall Accuracy: {overall_accuracy:.2%}")
        print(f"   Language Detection: {overall_language_detection:.2%}")
        print(f"   Languages Tested: {len(romance_languages)}")
        
        # Validation
        assert overall_accuracy >= 0.8, f"Romance languages accuracy too low: {overall_accuracy:.2%}"
        assert overall_language_detection >= 0.9, f"Language detection too low: {overall_language_detection:.2%}"
        
        self.multilingual_metrics["romance_languages"] = {
            "overall_accuracy": overall_accuracy,
            "language_detection": overall_language_detection,
            "language_results": language_results
        }
        
        self.demo_results["romance_languages"] = "PASS"
        
    def test_asian_languages_accuracy(self):
        """Demo: Test accuracy on Asian languages (Chinese, Japanese)."""
        print(f"\n=== DEMO: Asian Languages Accuracy Test ===")
        
        asian_languages = ["chinese_simplified", "japanese"]
        all_results = []
        language_results = {}
        
        for lang_key in asian_languages:
            language_data = self.multilingual_documents[lang_key]
            results = []
            
            print(f"\nTesting {language_data['language']}:")
            
            for doc_type, document in language_data["documents"].items():
                # Analyze document with special Asian language handling
                analysis = self._simulate_multilingual_analysis(
                    document["content"], 
                    language_data["language_code"],
                    special_handling="asian"
                )
                
                # Validate results
                correct_prediction = analysis["has_arbitration"] == document["expected_arbitration"]
                confidence_met = analysis["confidence"] >= document["confidence_threshold"]
                character_encoding_ok = analysis["character_encoding_confidence"] > 0.9
                
                result = {
                    "document_type": doc_type,
                    "language": language_data["language"],
                    "expected": document["expected_arbitration"],
                    "predicted": analysis["has_arbitration"],
                    "confidence": analysis["confidence"],
                    "correct_prediction": correct_prediction,
                    "confidence_met": confidence_met,
                    "character_encoding_ok": character_encoding_ok,
                    "segmentation_quality": analysis["segmentation_quality"]
                }
                
                results.append(result)
                all_results.append(result)
                
                status = "âœ“" if correct_prediction and confidence_met else "âœ—"
                print(f"  {status} {doc_type}: {analysis['has_arbitration']} ({analysis['confidence']:.2f})")
                print(f"     Character encoding: {analysis['character_encoding_confidence']:.2f}")
                print(f"     Segmentation quality: {analysis['segmentation_quality']:.2f}")
                
            # Calculate language-specific metrics
            accuracy = sum(1 for r in results if r["correct_prediction"]) / len(results)
            encoding_quality = sum(1 for r in results if r["character_encoding_ok"]) / len(results)
            
            language_results[lang_key] = {
                "accuracy": accuracy,
                "encoding_quality": encoding_quality,
                "results": results
            }
            
            print(f"   Language accuracy: {accuracy:.2%}")
            print(f"   Encoding quality: {encoding_quality:.2%}")
            
        # Calculate overall Asian language metrics
        overall_accuracy = sum(1 for r in all_results if r["correct_prediction"]) / len(all_results)
        overall_encoding = sum(1 for r in all_results if r["character_encoding_ok"]) / len(all_results)
        
        print(f"\nðŸ“Š Asian Languages Overall Results:")
        print(f"   Overall Accuracy: {overall_accuracy:.2%}")
        print(f"   Character Encoding Quality: {overall_encoding:.2%}")
        print(f"   Languages Tested: {len(asian_languages)}")
        
        # Validation (slightly lower thresholds for complex scripts)
        assert overall_accuracy >= 0.75, f"Asian languages accuracy too low: {overall_accuracy:.2%}"
        assert overall_encoding >= 0.9, f"Character encoding quality too low: {overall_encoding:.2%}"
        
        self.multilingual_metrics["asian_languages"] = {
            "overall_accuracy": overall_accuracy,
            "encoding_quality": overall_encoding,
            "language_results": language_results
        }
        
        self.demo_results["asian_languages"] = "PASS"
        
    def test_arabic_script_accuracy(self):
        """Demo: Test accuracy on Arabic script and RTL text processing."""
        print(f"\n=== DEMO: Arabic Script and RTL Processing Test ===")
        
        language_data = self.multilingual_documents["arabic"]
        results = []
        
        print(f"Testing {language_data['language']} (Right-to-Left script):")
        
        for doc_type, document in language_data["documents"].items():
            # Analyze document with RTL-specific handling
            analysis = self._simulate_multilingual_analysis(
                document["content"], 
                language_data["language_code"],
                special_handling="rtl"
            )
            
            # Validate results
            correct_prediction = analysis["has_arbitration"] == document["expected_arbitration"]
            confidence_met = analysis["confidence"] >= document["confidence_threshold"]
            rtl_processing_ok = analysis["rtl_processing_confidence"] > 0.8
            
            result = {
                "document_type": doc_type,
                "language": language_data["language"],
                "expected": document["expected_arbitration"],
                "predicted": analysis["has_arbitration"],
                "confidence": analysis["confidence"],
                "correct_prediction": correct_prediction,
                "confidence_met": confidence_met,
                "rtl_processing_ok": rtl_processing_ok,
                "text_direction": analysis["text_direction"],
                "script_confidence": analysis["script_confidence"]
            }
            
            results.append(result)
            
            status = "âœ“" if correct_prediction and confidence_met else "âœ—"
            print(f"  {status} {doc_type}: {analysis['has_arbitration']} ({analysis['confidence']:.2f})")
            print(f"     RTL processing: {analysis['rtl_processing_confidence']:.2f}")
            print(f"     Script confidence: {analysis['script_confidence']:.2f}")
            print(f"     Text direction: {analysis['text_direction']}")
            
        # Calculate Arabic-specific metrics
        accuracy = sum(1 for r in results if r["correct_prediction"]) / len(results)
        rtl_quality = sum(1 for r in results if r["rtl_processing_ok"]) / len(results)
        
        print(f"\nðŸ“Š Arabic Script Results:")
        print(f"   Accuracy: {accuracy:.2%}")
        print(f"   RTL Processing Quality: {rtl_quality:.2%}")
        print(f"   Script Recognition: Arabic")
        
        # Validation
        assert accuracy >= 0.75, f"Arabic accuracy too low: {accuracy:.2%}"
        assert rtl_quality >= 0.8, f"RTL processing quality too low: {rtl_quality:.2%}"
        
        self.multilingual_metrics["arabic_script"] = {
            "accuracy": accuracy,
            "rtl_quality": rtl_quality,
            "results": results
        }
        
        self.demo_results["arabic_script"] = "PASS"
        
    def test_cross_language_consistency(self):
        """Demo: Test consistency of arbitration detection across languages."""
        print(f"\n=== DEMO: Cross-Language Consistency Test ===")
        
        # Test the same arbitration concept across different languages
        arbitration_results = []
        non_arbitration_results = []
        
        for lang_key, language_data in self.multilingual_documents.items():
            # Test arbitration documents
            arb_doc = language_data["documents"]["clear_arbitration"]
            arb_analysis = self._simulate_multilingual_analysis(
                arb_doc["content"],
                language_data["language_code"]
            )
            
            arbitration_results.append({
                "language": language_data["language"],
                "language_code": language_data["language_code"],
                "has_arbitration": arb_analysis["has_arbitration"],
                "confidence": arb_analysis["confidence"]
            })
            
            # Test non-arbitration documents
            non_arb_doc = language_data["documents"]["no_arbitration"]
            non_arb_analysis = self._simulate_multilingual_analysis(
                non_arb_doc["content"],
                language_data["language_code"]
            )
            
            non_arbitration_results.append({
                "language": language_data["language"],
                "language_code": language_data["language_code"],
                "has_arbitration": non_arb_analysis["has_arbitration"],
                "confidence": non_arb_analysis["confidence"]
            })
            
        # Analyze consistency
        arb_predictions = [r["has_arbitration"] for r in arbitration_results]
        non_arb_predictions = [r["has_arbitration"] for r in non_arbitration_results]
        
        arb_consistency = len(set(arb_predictions)) == 1 and arb_predictions[0] == True
        non_arb_consistency = len(set(non_arb_predictions)) == 1 and non_arb_predictions[0] == False
        
        # Calculate confidence variation
        arb_confidences = [r["confidence"] for r in arbitration_results]
        non_arb_confidences = [r["confidence"] for r in non_arbitration_results]
        
        import statistics
        arb_confidence_std = statistics.stdev(arb_confidences) if len(arb_confidences) > 1 else 0
        non_arb_confidence_std = statistics.stdev(non_arb_confidences) if len(non_arb_confidences) > 1 else 0
        
        print(f"Arbitration Document Results:")
        for result in arbitration_results:
            status = "âœ“" if result["has_arbitration"] else "âœ—"
            print(f"  {status} {result['language']}: {result['has_arbitration']} ({result['confidence']:.2f})")
            
        print(f"\nNon-Arbitration Document Results:")
        for result in non_arbitration_results:
            status = "âœ“" if not result["has_arbitration"] else "âœ—"
            print(f"  {status} {result['language']}: {result['has_arbitration']} ({result['confidence']:.2f})")
            
        print(f"\nðŸ“Š Cross-Language Consistency Results:")
        print(f"   Arbitration Consistency: {arb_consistency}")
        print(f"   Non-Arbitration Consistency: {non_arb_consistency}")
        print(f"   Arbitration Confidence Std Dev: {arb_confidence_std:.3f}")
        print(f"   Non-Arbitration Confidence Std Dev: {non_arb_confidence_std:.3f}")
        print(f"   Languages Tested: {len(self.multilingual_documents)}")
        
        # Validation
        assert arb_consistency, "Arbitration detection not consistent across languages"
        assert non_arb_consistency, "Non-arbitration detection not consistent across languages"
        assert arb_confidence_std < 0.2, f"Arbitration confidence variation too high: {arb_confidence_std:.3f}"
        assert non_arb_confidence_std < 0.2, f"Non-arbitration confidence variation too high: {non_arb_confidence_std:.3f}"
        
        self.multilingual_metrics["cross_language_consistency"] = {
            "arbitration_consistency": arb_consistency,
            "non_arbitration_consistency": non_arb_consistency,
            "arbitration_confidence_std": arb_confidence_std,
            "non_arbitration_confidence_std": non_arb_confidence_std,
            "arbitration_results": arbitration_results,
            "non_arbitration_results": non_arbitration_results
        }
        
        self.demo_results["cross_language_consistency"] = "PASS"
        
    def test_mixed_language_documents(self):
        """Demo: Test handling of documents with mixed languages."""
        print(f"\n=== DEMO: Mixed Language Documents Test ===")
        
        mixed_language_docs = [
            {
                "id": "english_spanish_mix",
                "content": """
                CONTRATO / CONTRACT
                
                ARBITRATION CLAUSE / CLÃUSULA DE ARBITRAJE:
                Any dispute arising from this agreement / Cualquier disputa que surja de este acuerdo
                shall be resolved through binding arbitration / serÃ¡ resuelta mediante arbitraje vinculante
                administered by AAA / administrado por AAA.
                """,
                "primary_language": "en",
                "secondary_language": "es",
                "expected_arbitration": True
            },
            {
                "id": "french_english_mix",
                "content": """
                ACCORD / AGREEMENT
                
                RÃ‰SOLUTION DES DIFFÃ‰RENDS / DISPUTE RESOLUTION:
                All disputes will be resolved in French courts / Tous les diffÃ©rends seront rÃ©solus 
                devant les tribunaux franÃ§ais in accordance with French law / conformÃ©ment au droit franÃ§ais.
                """,
                "primary_language": "fr",
                "secondary_language": "en",
                "expected_arbitration": False
            },
            {
                "id": "multilingual_arbitration",
                "content": """
                ä»²è£æ¡æ¬¾ ARBITRATION CLAUSE CLÃUSULA DE ARBITRAJE
                
                äº‰è®®è§£å†³ All disputes shall be resolved through arbitration Las disputas serÃ¡n resueltas por arbitraje
                administered by å›½é™…ä»²è£é™¢ ICC International Arbitration ä»²è£è§„åˆ™ Rules.
                """,
                "primary_language": "zh",
                "secondary_language": "en",
                "tertiary_language": "es",
                "expected_arbitration": True
            }
        ]
        
        results = []
        
        for doc in mixed_language_docs:
            print(f"\nTesting: {doc['id']}")
            
            # Analyze mixed language document
            analysis = self._simulate_multilingual_analysis(
                doc["content"],
                doc["primary_language"],
                special_handling="mixed_language"
            )
            
            # Validate results
            correct_prediction = analysis["has_arbitration"] == doc["expected_arbitration"]
            primary_lang_detected = doc["primary_language"] in analysis["detected_languages"]
            secondary_lang_detected = doc["secondary_language"] in analysis["detected_languages"]
            
            result = {
                "document_id": doc["id"],
                "expected": doc["expected_arbitration"],
                "predicted": analysis["has_arbitration"],
                "confidence": analysis["confidence"],
                "correct_prediction": correct_prediction,
                "primary_lang_detected": primary_lang_detected,
                "secondary_lang_detected": secondary_lang_detected,
                "detected_languages": analysis["detected_languages"],
                "language_mixing_confidence": analysis["language_mixing_confidence"]
            }
            
            results.append(result)
            
            status = "âœ“" if correct_prediction else "âœ—"
            print(f"{status} Prediction: {analysis['has_arbitration']} ({analysis['confidence']:.2f})")
            print(f"   Detected languages: {', '.join(analysis['detected_languages'])}")
            print(f"   Language mixing confidence: {analysis['language_mixing_confidence']:.2f}")
            
        # Calculate mixed language metrics
        accuracy = sum(1 for r in results if r["correct_prediction"]) / len(results)
        primary_detection = sum(1 for r in results if r["primary_lang_detected"]) / len(results)
        secondary_detection = sum(1 for r in results if r["secondary_lang_detected"]) / len(results)
        
        print(f"\nðŸ“Š Mixed Language Documents Results:")
        print(f"   Accuracy: {accuracy:.2%}")
        print(f"   Primary Language Detection: {primary_detection:.2%}")
        print(f"   Secondary Language Detection: {secondary_detection:.2%}")
        print(f"   Documents Tested: {len(mixed_language_docs)}")
        
        # Validation
        assert accuracy >= 0.75, f"Mixed language accuracy too low: {accuracy:.2%}"
        assert primary_detection >= 0.8, f"Primary language detection too low: {primary_detection:.2%}"
        
        self.multilingual_metrics["mixed_language"] = {
            "accuracy": accuracy,
            "primary_detection": primary_detection,
            "secondary_detection": secondary_detection,
            "results": results
        }
        
        self.demo_results["mixed_language_documents"] = "PASS"
        
    def test_cultural_legal_context_adaptation(self):
        """Demo: Test adaptation to different cultural and legal contexts."""
        print(f"\n=== DEMO: Cultural and Legal Context Adaptation Test ===")
        
        cultural_contexts = {
            "common_law": {
                "languages": ["english"],
                "legal_traditions": ["Anglo-American"],
                "arbitration_providers": ["AAA", "JAMS"],
                "expected_features": ["class_action_waiver", "jury_trial_waiver"]
            },
            "civil_law": {
                "languages": ["french", "german", "spanish"],
                "legal_traditions": ["Continental European"],
                "arbitration_providers": ["ICC", "DIS"],
                "expected_features": ["institutional_arbitration", "seat_specification"]
            },
            "mixed_systems": {
                "languages": ["chinese_simplified", "japanese"],
                "legal_traditions": ["East Asian"],
                "arbitration_providers": ["CIETAC", "JCAA"],
                "expected_features": ["language_specification", "cultural_considerations"]
            },
            "sharia_influenced": {
                "languages": ["arabic"],
                "legal_traditions": ["Islamic Law"],
                "arbitration_providers": ["Gulf Arbitration"],
                "expected_features": ["religious_compliance", "cultural_sensitivity"]
            }
        }
        
        context_results = {}
        
        for context_type, context_info in cultural_contexts.items():
            print(f"\nTesting {context_type.replace('_', ' ').title()} Context:")
            
            context_languages = [lang for lang in context_info["languages"] if lang in self.multilingual_documents]
            results = []
            
            for lang_key in context_languages:
                language_data = self.multilingual_documents[lang_key]
                doc = language_data["documents"]["clear_arbitration"]
                
                # Analyze with cultural context awareness
                analysis = self._simulate_multilingual_analysis(
                    doc["content"],
                    language_data["language_code"],
                    cultural_context=context_type
                )
                
                # Check cultural adaptation
                cultural_features = self._analyze_cultural_features(doc["content"], context_type)
                
                result = {
                    "language": language_data["language"],
                    "has_arbitration": analysis["has_arbitration"],
                    "confidence": analysis["confidence"],
                    "cultural_features": cultural_features,
                    "context_adaptation_score": analysis["context_adaptation_score"]
                }
                
                results.append(result)
                
                print(f"  {language_data['language']}: {analysis['has_arbitration']} ({analysis['confidence']:.2f})")
                print(f"    Cultural adaptation: {analysis['context_adaptation_score']:.2f}")
                print(f"    Features detected: {', '.join(cultural_features)}")
                
            # Calculate context-specific metrics
            avg_confidence = sum(r["confidence"] for r in results) / len(results) if results else 0
            avg_adaptation = sum(r["context_adaptation_score"] for r in results) / len(results) if results else 0
            
            context_results[context_type] = {
                "average_confidence": avg_confidence,
                "average_adaptation": avg_adaptation,
                "languages_tested": len(results),
                "results": results
            }
            
            print(f"  Context Summary:")
            print(f"    Average Confidence: {avg_confidence:.2f}")
            print(f"    Average Adaptation: {avg_adaptation:.2f}")
            
        print(f"\nðŸ“Š Cultural Context Adaptation Results:")
        for context, metrics in context_results.items():
            print(f"   {context.replace('_', ' ').title()}:")
            print(f"     Languages: {metrics['languages_tested']}")
            print(f"     Avg Confidence: {metrics['average_confidence']:.2f}")
            print(f"     Adaptation Score: {metrics['average_adaptation']:.2f}")
            
        # Validation
        for context, metrics in context_results.items():
            assert metrics["average_confidence"] >= 0.7, f"{context} confidence too low: {metrics['average_confidence']:.2f}"
            assert metrics["average_adaptation"] >= 0.7, f"{context} adaptation too low: {metrics['average_adaptation']:.2f}"
            
        self.multilingual_metrics["cultural_adaptation"] = context_results
        
        self.demo_results["cultural_legal_context"] = "PASS"
        
    # Helper methods
    def _simulate_multilingual_analysis(self, content: str, language_code: str, 
                                      special_handling: str = None, 
                                      cultural_context: str = None) -> Dict[str, Any]:
        """Simulate multilingual arbitration analysis."""
        time.sleep(0.15)  # Simulate processing time
        
        # Language detection simulation
        language_map = {
            "en": "English",
            "es": "Spanish", 
            "fr": "French",
            "de": "German",
            "zh-CN": "Chinese (Simplified)",
            "ja": "Japanese",
            "pt": "Portuguese",
            "ar": "Arabic"
        }
        
        detected_language = language_map.get(language_code, "Unknown")
        language_confidence = 0.95 if language_code in language_map else 0.3
        
        # Multilingual keyword analysis
        multilingual_keywords = {
            "arbitration": ["arbitration", "arbitraje", "arbitrage", "schiedsgerichtsbarkeit", "ä»²è£", "ä»²è£", "arbitragem", "ØªØ­ÙƒÙŠÙ…"],
            "binding": ["binding", "vinculante", "contraignant", "verbindlich", "æœ‰çº¦æŸåŠ›", "æ‹˜æŸåŠ›", "vinculativo", "Ù…Ù„Ø²Ù…"],
            "dispute": ["dispute", "disputa", "diffÃ©rend", "streitigkeit", "äº‰è®®", "ç´›äº‰", "disputa", "Ù†Ø²Ø§Ø¹"]
        }
        
        # Analyze content for multilingual keywords
        content_lower = content.lower()
        found_keywords = []
        keyword_scores = {}
        
        for concept, translations in multilingual_keywords.items():
            for translation in translations:
                if translation in content_lower:
                    found_keywords.append(translation)
                    keyword_scores[concept] = keyword_scores.get(concept, 0) + 1
                    
        # Calculate confidence based on multilingual analysis
        base_confidence = 0.1
        if keyword_scores.get("arbitration", 0) > 0:
            base_confidence += 0.4
        if keyword_scores.get("binding", 0) > 0:
            base_confidence += 0.3
        if keyword_scores.get("dispute", 0) > 0:
            base_confidence += 0.2
            
        # Adjust for language-specific factors
        if language_code in ["zh-CN", "ja", "ar"]:
            base_confidence *= 0.9  # Slightly lower confidence for complex scripts
        elif language_code in ["es", "fr", "pt"]:
            base_confidence *= 0.95  # Romance languages
            
        # Special handling adjustments
        special_metrics = {}
        
        if special_handling == "asian":
            special_metrics.update({
                "character_encoding_confidence": 0.95,
                "segmentation_quality": 0.9
            })
        elif special_handling == "rtl":
            special_metrics.update({
                "rtl_processing_confidence": 0.85,
                "text_direction": "rtl",
                "script_confidence": 0.9
            })
        elif special_handling == "mixed_language":
            detected_languages = []
            if any(kw in content_lower for kw in ["arbitration", "dispute", "binding"]):
                detected_languages.append("en")
            if any(kw in content_lower for kw in ["arbitraje", "disputa", "vinculante"]):
                detected_languages.append("es")
            if any(kw in content_lower for kw in ["arbitrage", "diffÃ©rend", "contraignant"]):
                detected_languages.append("fr")
            if any(kw in content_lower for kw in ["ä»²è£", "äº‰è®®"]):
                detected_languages.append("zh")
                
            special_metrics.update({
                "detected_languages": detected_languages,
                "language_mixing_confidence": 0.8 if len(detected_languages) > 1 else 0.3
            })
            
        # Cultural context adaptation
        context_adaptation_score = 0.8
        if cultural_context:
            # Simulate cultural adaptation
            if cultural_context == "common_law" and language_code == "en":
                context_adaptation_score = 0.95
            elif cultural_context == "civil_law" and language_code in ["fr", "de", "es"]:
                context_adaptation_score = 0.9
            elif cultural_context == "mixed_systems" and language_code in ["zh-CN", "ja"]:
                context_adaptation_score = 0.85
            elif cultural_context == "sharia_influenced" and language_code == "ar":
                context_adaptation_score = 0.8
                
        # Determine arbitration presence
        has_arbitration = len(found_keywords) >= 2 and keyword_scores.get("arbitration", 0) > 0
        
        # Final confidence calculation
        confidence = min(0.98, max(0.05, base_confidence))
        
        result = {
            "has_arbitration": has_arbitration,
            "confidence": confidence,
            "detected_language": detected_language,
            "language_confidence": language_confidence,
            "cross_language_keywords": found_keywords,
            "context_adaptation_score": context_adaptation_score
        }
        
        # Add special handling metrics
        result.update(special_metrics)
        
        return result
        
    def _analyze_cultural_features(self, content: str, context_type: str) -> List[str]:
        """Analyze cultural and legal features in the content."""
        features = []
        content_lower = content.lower()
        
        # Common law features
        if context_type == "common_law":
            if "class action" in content_lower or "class waiver" in content_lower:
                features.append("class_action_waiver")
            if "jury trial" in content_lower:
                features.append("jury_trial_waiver")
            if "aaa" in content_lower or "jams" in content_lower:
                features.append("us_arbitration_provider")
                
        # Civil law features
        elif context_type == "civil_law":
            if "icc" in content_lower or "dis" in content_lower:
                features.append("institutional_arbitration")
            if any(city in content_lower for city in ["paris", "berlin", "madrid"]):
                features.append("seat_specification")
                
        # Mixed systems features
        elif context_type == "mixed_systems":
            if any(provider in content_lower for provider in ["cietac", "jcaa"]):
                features.append("regional_arbitration_provider")
            if any(lang in content_lower for lang in ["chinese", "japanese", "english"]):
                features.append("language_specification")
                
        # Sharia-influenced features
        elif context_type == "sharia_influenced":
            if "dubai" in content_lower or "gulf" in content_lower:
                features.append("regional_compliance")
            if "arabic" in content_lower:
                features.append("cultural_sensitivity")
                
        return features
        
    def generate_multilingual_report(self) -> Dict[str, Any]:
        """Generate comprehensive multilingual testing report."""
        total_demos = len(self.demo_results)
        passed_demos = sum(1 for result in self.demo_results.values() if result == "PASS")
        
        # Calculate overall multilingual score
        multilingual_score = 0
        score_weights = {
            "english_baseline": 0.15,
            "romance_languages": 0.25,
            "asian_languages": 0.25,
            "arabic_script": 0.15,
            "cross_language_consistency": 0.1,
            "mixed_language": 0.05,
            "cultural_adaptation": 0.05
        }
        
        for metric_key, weight in score_weights.items():
            if metric_key in self.multilingual_metrics:
                metric = self.multilingual_metrics[metric_key]
                if "accuracy" in metric:
                    score = metric["accuracy"]
                elif "overall_accuracy" in metric:
                    score = metric["overall_accuracy"]
                elif "arbitration_consistency" in metric and "non_arbitration_consistency" in metric:
                    score = (metric["arbitration_consistency"] + metric["non_arbitration_consistency"]) / 2
                else:
                    score = 0.8  # Default score
                    
                multilingual_score += score * weight
                
        return {
            "multilingual_demo_report": {
                "total_demos": total_demos,
                "passed": passed_demos,
                "failed": total_demos - passed_demos,
                "success_rate": passed_demos / total_demos if total_demos > 0 else 0,
                "multilingual_score": multilingual_score,
                "languages_tested": len(self.multilingual_documents),
                "multilingual_metrics": self.multilingual_metrics,
                "demo_results": self.demo_results,
                "timestamp": time.time(),
                "supported_languages": list(self.multilingual_documents.keys()),
                "cultural_contexts_tested": ["common_law", "civil_law", "mixed_systems", "sharia_influenced"]
            }
        }


if __name__ == "__main__":
    # Run multilingual demo tests
    pytest.main([__file__, "-v", "--tb=short", "-s"])