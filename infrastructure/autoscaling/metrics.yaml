# Auto-scaling Configuration for Enterprise Performance
# Supports 10,000+ concurrent users with predictive scaling

# Kubernetes HPA (Horizontal Pod Autoscaler) Configuration
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-autoscaler
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  
  minReplicas: 5
  maxReplicas: 100
  
  # Scaling metrics
  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # Scale when CPU > 70%
    
    # Memory-based scaling  
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80  # Scale when memory > 80%
    
    # Custom metrics from application
    - type: Pods
      pods:
        metric:
          name: response_time_p95
        target:
          type: AverageValue
          averageValue: "500m"  # Scale when P95 > 500ms
    
    - type: Pods
      pods:
        metric:
          name: requests_per_second
        target:
          type: AverageValue
          averageValue: "100"  # Scale when RPS > 100 per pod
    
    - type: External
      external:
        metric:
          name: queue_depth
          selector:
            matchLabels:
              queue: "main-processing"
        target:
          type: Value
          value: "1000"  # Scale when queue depth > 1000
    
    - type: External
      external:
        metric:
          name: database_connections
          selector:
            matchLabels:
              database: "primary"
        target:
          type: AverageValue
          averageValue: "80"  # Scale when DB connections > 80%
  
  # Scaling behavior
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
        - type: Percent
          value: 10  # Scale down by 10% at a time
          periodSeconds: 60
        - type: Pods
          value: 2  # Remove max 2 pods at a time
          periodSeconds: 60
      selectPolicy: Min  # Use the most conservative policy
    
    scaleUp:
      stabilizationWindowSeconds: 60  # Scale up quickly (1 minute)
      policies:
        - type: Percent
          value: 100  # Can double the pods
          periodSeconds: 60
        - type: Pods
          value: 10  # Add max 10 pods at a time
          periodSeconds: 60
      selectPolicy: Max  # Use the most aggressive policy

---
# Vertical Pod Autoscaler Configuration
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  
  updatePolicy:
    updateMode: "Auto"  # Automatically update pod resources
  
  resourcePolicy:
    containerPolicies:
      - containerName: app
        minAllowed:
          cpu: 500m
          memory: 512Mi
        maxAllowed:
          cpu: 4
          memory: 8Gi
        controlledResources: ["cpu", "memory"]
        controlledValues: RequestsAndLimits

---
# AWS Auto Scaling Configuration
AWSAutoScaling:
  # EC2 Auto Scaling Group
  AutoScalingGroup:
    Name: app-asg-production
    MinSize: 5
    MaxSize: 100
    DesiredCapacity: 10
    
    # Instance configuration
    LaunchTemplate:
      InstanceType: c5.2xlarge  # 8 vCPU, 16 GB RAM
      SpotInstancePools: 3  # Use spot instances for cost optimization
      OnDemandPercentage: 30  # 30% on-demand, 70% spot
    
    # Health checks
    HealthCheckType: ELB
    HealthCheckGracePeriod: 300
    
    # Scaling policies
    TargetTrackingScalingPolicies:
      - PolicyName: cpu-target-tracking
        TargetValue: 70.0
        PredefinedMetricType: ASGAverageCPUUtilization
      
      - PolicyName: request-count-tracking
        TargetValue: 1000
        CustomizedMetricSpecification:
          MetricName: RequestCountPerTarget
          Namespace: AWS/ApplicationELB
          Statistic: Average
      
      - PolicyName: response-time-tracking
        TargetValue: 0.5  # 500ms
        CustomizedMetricSpecification:
          MetricName: TargetResponseTime
          Namespace: AWS/ApplicationELB
          Statistic: Average
    
    # Step scaling for rapid response
    StepScalingPolicies:
      - PolicyName: aggressive-scale-up
        AdjustmentType: PercentChangeInCapacity
        MetricAggregationType: Average
        StepAdjustments:
          - MetricIntervalLowerBound: 0
            MetricIntervalUpperBound: 10
            ScalingAdjustment: 10
          - MetricIntervalLowerBound: 10
            MetricIntervalUpperBound: 20
            ScalingAdjustment: 20
          - MetricIntervalLowerBound: 20
            ScalingAdjustment: 50
    
    # Predictive scaling
    PredictiveScalingPolicies:
      - PolicyName: ml-based-scaling
        Mode: ForecastAndScale
        MetricSpecifications:
          - TargetValue: 70
            PredefinedMetricPairSpecification:
              PredefinedMetricType: ASGCPUUtilization
          - TargetValue: 1000
            CustomizedLoadMetricSpecification:
              MetricDataQueries:
                - Id: load_metric
                  MetricStat:
                    Metric:
                      MetricName: RequestCount
                      Namespace: AWS/ApplicationELB
                    Stat: Sum
        SchedulingBufferTime: 120  # Start scaling 2 minutes before predicted load

  # Application Auto Scaling (for ECS/Lambda)
  ApplicationAutoScaling:
    ServiceNamespace: ecs
    ResourceId: service/app-cluster/app-service
    ScalableDimension: ecs:service:DesiredCount
    MinCapacity: 10
    MaxCapacity: 200
    
    TargetTrackingScalingPolicies:
      - PolicyName: ecs-cpu-scaling
        TargetValue: 75.0
        PredefinedMetricType: ECSServiceAverageCPUUtilization
        ScaleOutCooldown: 60
        ScaleInCooldown: 300
      
      - PolicyName: ecs-memory-scaling
        TargetValue: 80.0
        PredefinedMetricType: ECSServiceAverageMemoryUtilization

  # Lambda Concurrency Scaling
  LambdaConcurrency:
    FunctionName: app-api-handler
    ReservedConcurrentExecutions: 100
    ProvisionedConcurrencyConfig:
      AllocatedConcurrentExecutions: 50
      
    ApplicationAutoScaling:
      MinCapacity: 50
      MaxCapacity: 1000
      TargetTrackingScalingPolicy:
        TargetValue: 0.7
        PredefinedMetricType: LambdaProvisionedConcurrencyUtilization

---
# Google Cloud Auto-scaling Configuration
GCPAutoScaling:
  # Instance Group Autoscaler
  InstanceGroupAutoscaler:
    name: app-autoscaler
    target: instance-groups/app-ig
    
    autoscalingPolicy:
      minNumReplicas: 5
      maxNumReplicas: 100
      coolDownPeriodSec: 120
      
      # CPU-based autoscaling
      cpuUtilization:
        utilizationTarget: 0.7
        predictiveMethod: OPTIMIZE_AVAILABILITY
      
      # Custom metrics
      customMetricUtilizations:
        - metric: custom.googleapis.com/response_time_p95
          utilizationTarget: 500
          utilizationTargetType: GAUGE
        
        - metric: custom.googleapis.com/queue_depth
          utilizationTarget: 1000
          utilizationTargetType: GAUGE
      
      # Load balancing utilization
      loadBalancingUtilization:
        utilizationTarget: 0.8
      
      # Scaling schedules
      scaleInControl:
        maxScaledInReplicas:
          fixed: 10  # Don't scale in more than 10 instances at once
        timeWindowSec: 600  # Over 10 minutes
      
      scaleOutControl:
        maxScaledOutReplicas:
          percent: 100  # Can double the instances
        timeWindowSec: 60  # In 1 minute

---
# Azure Autoscale Configuration
AzureAutoscale:
  # Virtual Machine Scale Set
  VMScaleSet:
    name: app-vmss
    resourceGroup: production-rg
    
    sku:
      name: Standard_D4s_v3  # 4 vCPUs, 16 GB RAM
      tier: Standard
      capacity: 10
    
    autoscaleSettings:
      name: app-autoscale-settings
      enabled: true
      
      profiles:
        # Default profile
        - name: default-profile
          capacity:
            minimum: 5
            maximum: 100
            default: 10
          
          rules:
            # CPU-based rule
            - metricTrigger:
                metricName: Percentage CPU
                metricResourceUri: /subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.Compute/virtualMachineScaleSets/app-vmss
                timeGrain: PT1M
                statistic: Average
                timeWindow: PT5M
                timeAggregation: Average
                operator: GreaterThan
                threshold: 70
              
              scaleAction:
                direction: Increase
                type: PercentChangeCount
                value: 20
                cooldown: PT5M
            
            # Memory-based rule
            - metricTrigger:
                metricName: Available Memory Bytes
                operator: LessThan
                threshold: 1073741824  # 1 GB
              
              scaleAction:
                direction: Increase
                type: ChangeCount
                value: 5
                cooldown: PT5M
            
            # Application Insights metric
            - metricTrigger:
                metricName: requests/sec
                metricNamespace: microsoft.insights/components
                operator: GreaterThan
                threshold: 1000
              
              scaleAction:
                direction: Increase
                type: PercentChangeCount
                value: 50
                cooldown: PT2M
        
        # Schedule-based profile for peak hours
        - name: peak-hours-profile
          capacity:
            minimum: 20
            maximum: 100
            default: 30
          
          fixedDate:
            timeZone: UTC
            start: "2024-01-01T09:00:00Z"
            end: "2024-12-31T17:00:00Z"
          
          recurrence:
            frequency: Week
            schedule:
              days: ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
              hours: [9]
              minutes: [0]
      
      notifications:
        - operation: Scale
          email:
            sendToSubscriptionAdministrator: true
            customEmails:
              - ops-team@example.com
          
          webhooks:
            - serviceUri: https://monitoring.example.com/autoscale-webhook

---
# Custom Metrics Configuration
CustomMetrics:
  # Prometheus metrics for scaling decisions
  PrometheusMetrics:
    - name: http_requests_per_second
      query: rate(http_requests_total[1m])
      threshold: 100
      
    - name: response_time_p95
      query: histogram_quantile(0.95, http_request_duration_seconds_bucket[5m])
      threshold: 0.5
      
    - name: error_rate
      query: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])
      threshold: 0.01
      
    - name: queue_depth
      query: rabbitmq_queue_messages_ready{queue="main"}
      threshold: 1000
      
    - name: database_pool_exhaustion
      query: (pg_stat_database_numbackends / pg_settings_max_connections) * 100
      threshold: 80
  
  # DataDog metrics
  DataDogMetrics:
    - name: custom.app.concurrent_users
      threshold: 10000
      
    - name: custom.app.cache_hit_ratio
      threshold: 0.8
      operator: LessThan  # Scale if cache hit ratio drops
  
  # CloudWatch custom metrics
  CloudWatchMetrics:
    - namespace: CustomApp
      metricName: ActiveSessions
      dimensions:
        - name: Environment
          value: Production
      threshold: 5000
      
    - namespace: CustomApp
      metricName: TransactionRate
      threshold: 1000

---
# Machine Learning Based Scaling
MLScaling:
  enabled: true
  
  model:
    type: LSTM  # Long Short-Term Memory for time series
    updateFrequency: daily
    
    features:
      - historical_load_1h
      - historical_load_24h
      - historical_load_7d
      - day_of_week
      - hour_of_day
      - special_events
      - trending_coefficient
    
    prediction_window: 30m  # Predict 30 minutes ahead
    confidence_threshold: 0.85
  
  # Anomaly detection for unexpected spikes
  anomalyDetection:
    enabled: true
    algorithm: IsolationForest
    sensitivity: high
    
    actions:
      - type: aggressive_scale
        trigger: anomaly_score > 0.8
        scale_factor: 2.0
      
      - type: alert
        trigger: anomaly_score > 0.9
        channels:
          - slack
          - pagerduty
  
  # Seasonal patterns
  seasonalPatterns:
    - name: black_friday
      date_range: [2024-11-29, 2024-12-02]
      preemptive_scale_factor: 3.0
      
    - name: end_of_month
      cron: "0 0 28-31 * *"
      preemptive_scale_factor: 1.5
    
    - name: business_hours
      cron: "0 9-17 * * MON-FRI"
      min_capacity_override: 20

---
# Cost Optimization Settings
CostOptimization:
  enabled: true
  
  spot_instance_ratio: 0.7  # Use 70% spot instances
  
  instance_types:
    preference_order:
      - t3.medium    # Cheapest
      - t3.large
      - c5.large
      - c5.xlarge
      - c5.2xlarge  # Most expensive
  
  scheduled_scaling:
    # Scale down during off-peak hours
    - schedule: "0 22 * * *"  # 10 PM
      min_capacity: 2
      max_capacity: 20
    
    # Scale up before business hours
    - schedule: "0 8 * * MON-FRI"  # 8 AM weekdays
      min_capacity: 10
      max_capacity: 100
  
  # Rightsizing recommendations
  rightsizing:
    enabled: true
    check_frequency: weekly
    cpu_threshold_for_downsize: 30  # If CPU < 30% consistently
    memory_threshold_for_downsize: 40  # If memory < 40% consistently